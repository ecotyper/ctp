

"""该程序需要完成以下几个操作：

1) 监听RabbitMQ队列，从中读取实时股票报价数据
2) 将实时股票报价数据写入到Redis高速缓存中

"""

import pika 
import time 
import json 
import logging 
import pandas as pd 
from redis_operation import RedisDataBridge


#import json
import os
from py_ctp.eventEngine import  *
from py_ctp.eventType import  *
from py_ctp.ctp_struct import *
from py_ctp.quote import Quote
from py_ctp.trade import Trade
import time



rabbitmq_host = "rabbitmq" 
live_data_exchange_name = "stock_price" 
live_data_queue_name = "stock_queue" 

result_exchange_name = "prediction_result" 
result_queue_name = "prediction_result_queue" 

## 设置日志等级，以便于在Docker输出中观察运行情况
logging.getLogger().setLevel(logging.INFO)    # logger = logging.getLogger()    # initialize logging class

                                              #logger.setLevel(logging.DEBUG)  # default log level
## logging包默认对输出分成了6个等级：日志级别等级:critical > error > warning > info > debug >


## 创建和RabbitMQ之间的连接，如果遇见连接失败的情况（可能是RabbitMQ的服
## 务器正在启动中），等待若干秒之后重试连接。
connection = pika.BlockingConnection(
    pika.ConnectionParameters(host = rabbitmq_host, 
                              connection_attempts = 10, 
                              retry_delay = 20))
logging.info("成功连接RabbitMQ %s" % rabbitmq_host) 

## 创建和RabbitMQ之间的频道：注意连接是一个独立TCP/IP协议连接，而频道是
## 其中的一个逻辑分隔。这一个频道将会专门为新的报价数据所用，我们后面将
## 会创建其他频道，为机器学习操作所用
channel_live_data = connection.channel()

## 创建一个交换中心，采用fanout模式
channel_live_data.exchange_declare(exchange = live_data_exchange_name, 
                                   exchange_type = "fanout")

## 创建一个队列，为该客户端独享
channel_live_data.queue_declare(queue = live_data_queue_name, 
                                exclusive = True) 

## 将队列和交换中心联系在一起
channel_live_data.queue_bind(exchange = live_data_exchange_name, 
                             queue = live_data_queue_name) 

## 创建和Redis高速缓存之间的连接: 
redis_data_bridge = RedisDataBridge("redis", read_length = 12) 
